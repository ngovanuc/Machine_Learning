{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8e0ea6",
   "metadata": {},
   "source": [
    "## üß† I. GBM l√† g√¨?\n",
    "\n",
    "**Gradient Boosting Machine (GBM)** l√† m·ªôt **thu·∫≠t to√°n boosting** t·∫°o ra m√¥ h√¨nh m·∫°nh t·ª´ **nhi·ªÅu m√¥ h√¨nh y·∫øu (th∆∞·ªùng l√† decision trees)** b·∫±ng c√°ch **h·ªçc t·ª´ l·ªói c·ªßa m√¥ h√¨nh tr∆∞·ªõc ƒë√≥**.\n",
    "\n",
    "> GBM = T·∫≠p h·ª£p c√°c c√¢y nh·ªè ‚Üí M·ªói c√¢y h·ªçc ƒë·ªÉ s·ª≠a l·ªói c·ªßa t·ªï h·ª£p tr∆∞·ªõc ƒë√≥\n",
    "> ‚Üí H·ªçc d·∫ßn d·∫ßn, theo h∆∞·ªõng **gi·∫£m loss** th√¥ng qua **gradient descent**.\n",
    "\n",
    "\n",
    "## üîÅ II. √ù t∆∞·ªüng t·ªïng qu√°t\n",
    "\n",
    "Gi·∫£ s·ª≠ b√†i to√°n h·ªìi quy (ph√¢n lo·∫°i c≈©ng √°p d·ª•ng t∆∞∆°ng t·ª±):\n",
    "\n",
    "1. Kh·ªüi t·∫°o m√¥ h√¨nh v·ªõi 1 gi√° tr·ªã ƒë∆°n gi·∫£n, v√≠ d·ª•:\n",
    "\n",
    "   $$\n",
    "   F_0(x) = \\arg\\min_c \\sum_{i=1}^n L(y_i, c)\n",
    "   $$\n",
    "\n",
    "2. V·ªõi m·ªói v√≤ng l·∫∑p $m = 1, 2, ..., M$:\n",
    "\n",
    "   * T√≠nh **residual**: ƒë·∫°o h√†m √¢m c·ªßa loss\n",
    "\n",
    "     $$\n",
    "     r_i^{(m)} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)}\n",
    "     $$\n",
    "   * Fit 1 **tree** $h_m(x)$ v√†o residual $r_i^{(m)}$\n",
    "   * C·∫≠p nh·∫≠t m√¥ h√¨nh:\n",
    "\n",
    "     $$\n",
    "     F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "     $$\n",
    "\n",
    "     V·ªõi $\\eta$: learning rate\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è III. C√°c th√†nh ph·∫ßn quan tr·ªçng\n",
    "\n",
    "| Th√†nh ph·∫ßn               | M√¥ t·∫£                                 |\n",
    "| ------------------------ | ------------------------------------- |\n",
    "| **Base Learner**         | Th∆∞·ªùng l√† c√¢y quy·∫øt ƒë·ªãnh nh·ªè (stump)  |\n",
    "| **Loss function**        | H·ªìi quy: MSE; Ph√¢n lo·∫°i: log-loss     |\n",
    "| **Gradient**             | D·∫´n h∆∞·ªõng vi·ªác h·ªçc t·ª´ng b∆∞·ªõc          |\n",
    "| **Learning rate (Œ∑)**    | ƒêi·ªÅu ch·ªânh m·ª©c h·ªçc t·ª´ng v√≤ng          |\n",
    "| **Number of estimators** | S·ªë l∆∞·ª£ng c√¢y (v√≤ng boosting)          |\n",
    "| **Subsampling**          | TƒÉng randomization, ch·ªëng overfitting |\n",
    "| **Max depth**            | ƒê·ªô s√¢u m·ªói c√¢y con                    |\n",
    "\n",
    "\n",
    "## üìä IV. ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm\n",
    "\n",
    "| ∆Øu ƒëi·ªÉm                                      | Nh∆∞·ª£c ƒëi·ªÉm                                    |\n",
    "| -------------------------------------------- | --------------------------------------------- |\n",
    "| M·∫°nh, th∆∞·ªùng ƒë·ª©ng top trong c√°c b√†i to√°n ML  | Hu·∫•n luy·ªán ch·∫≠m (sequential)                  |\n",
    "| X·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng r·∫•t t·ªët              | Nh·∫°y c·∫£m v·ªõi overfitting n·∫øu kh√¥ng regularize |\n",
    "| Linh ho·∫°t ‚Äì h·ªó tr·ª£ nhi·ªÅu loss function       | C·∫ßn tinh ch·ªânh nhi·ªÅu tham s·ªë                  |\n",
    "| C√≥ th·ªÉ m·ªü r·ªông (XGBoost, LightGBM, CatBoost) | Kh√≥ gi·∫£i th√≠ch h∆°n random forest              |\n",
    "\n",
    "\n",
    "## üß™ V. Code v√≠ d·ª• v·ªõi `sklearn.ensemble.GradientBoostingClassifier`\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "## üî• VI. C√°c bi·∫øn th·ªÉ hi·ªán ƒë·∫°i h∆°n c·ªßa GBM\n",
    "\n",
    "| T√™n          | ∆Øu ƒëi·ªÉm                       | ƒê·∫∑c tr∆∞ng                   |\n",
    "| ------------ | ----------------------------- | --------------------------- |\n",
    "| **XGBoost**  | Nhanh, regularization t·ªët     | H·ªó tr·ª£ pruning & parallel   |\n",
    "| **LightGBM** | R·∫•t nhanh, scale t·ªët          | Leaf-wise growth, histogram |\n",
    "| **CatBoost** | X·ª≠ l√Ω t·ªët categorical feature | Kh√¥ng c·∫ßn encode th·ªß c√¥ng   |\n",
    "\n",
    "\n",
    "## üîç VII. So s√°nh GBM v·ªõi c√°c m√¥ h√¨nh kh√°c\n",
    "\n",
    "| M√¥ h√¨nh            | So s√°nh                                             |\n",
    "| ------------------ | --------------------------------------------------- |\n",
    "| **Random Forest**  | Bagging song song vs Boosting tu·∫ßn t·ª±               |\n",
    "| **SVM**            | GBM t·ªët h∆°n v·ªõi feature nhi·ªÅu, SVM t·ªët v·ªõi √≠t chi·ªÅu |\n",
    "| **Neural Network** | GBM t·ªët v·ªõi tabular, NN t·ªët v·ªõi ·∫£nh, √¢m thanh       |\n",
    "| **Naive Bayes**    | GBM m·∫°nh h∆°n nhi·ªÅu, NB ch·ªâ m·∫°nh ·ªü text ƒë·∫∑c tr∆∞ng r√µ |\n",
    "\n",
    "\n",
    "## üéì T·ªïng k·∫øt\n",
    "\n",
    "| M·ª•c        | N·ªôi dung                                     |\n",
    "| ---------- | -------------------------------------------- |\n",
    "| Lo·∫°i       | Ensemble ‚Äì Boosting                          |\n",
    "| Th√†nh ph·∫ßn | Nhi·ªÅu c√¢y con h·ªçc tu·∫ßn t·ª±                    |\n",
    "| ∆Øu ƒëi·ªÉm    | M·∫°nh, linh ho·∫°t, ph√π h·ª£p d·ªØ li·ªáu d·∫°ng b·∫£ng   |\n",
    "| Nh∆∞·ª£c ƒëi·ªÉm | Hu·∫•n luy·ªán ch·∫≠m, d·ªÖ overfit n·∫øu kh√¥ng tuning |\n",
    "| Bi·∫øn th·ªÉ   | XGBoost, LightGBM, CatBoost                  |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
