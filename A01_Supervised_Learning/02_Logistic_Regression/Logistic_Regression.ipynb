{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980ff94e",
   "metadata": {},
   "source": [
    "## üìò I. Logistic Regression l√† g√¨?\n",
    "\n",
    "Logistic Regression l√† m√¥ h√¨nh h·ªçc c√≥ gi√°m s√°t d√πng ƒë·ªÉ **ph√¢n lo·∫°i** ƒë·∫ßu v√†o th√†nh c√°c l·ªõp. Kh√°c v·ªõi Linear Regression (cho ra gi√° tr·ªã li√™n t·ª•c), Logistic Regression cho ra **x√°c su·∫•t** ƒëi·ªÉm d·ªØ li·ªáu thu·ªôc v·ªÅ l·ªõp **d∆∞∆°ng (positive)**.\n",
    "\n",
    "\n",
    "## üßÆ II. √ù t∆∞·ªüng ch√≠nh\n",
    "\n",
    "Ta v·∫´n t√≠nh t·ªïng tuy·∫øn t√≠nh nh∆∞ Linear Regression:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Sau ƒë√≥, thay v√¨ d√πng tr·ª±c ti·∫øp $z$, ta ƒë∆∞a n√≥ qua **h√†m sigmoid** ƒë·ªÉ thu ƒë∆∞·ª£c x√°c su·∫•t:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "* $\\hat{y} \\in (0, 1)$: x√°c su·∫•t m·∫´u thu·ªôc l·ªõp 1\n",
    "* N·∫øu $\\hat{y} \\geq 0.5$, d·ª± ƒëo√°n l√† **l·ªõp 1**, ng∆∞·ª£c l·∫°i l√† **l·ªõp 0**\n",
    "\n",
    "\n",
    "## üìâ III. H√†m m·∫•t m√°t: **Binary Cross Entropy**\n",
    "\n",
    "Do output l√† x√°c su·∫•t, ta d√πng **log loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Loss n√†y ƒë·∫£m b·∫£o m√¥ h√¨nh **ph·∫°t m·∫°nh** n·∫øu d·ª± ƒëo√°n sai v·ªõi x√°c su·∫•t cao.\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è IV. T·ªëi ∆∞u ho√°: Gradient Descent\n",
    "\n",
    "Kh√¥ng c√≥ c√¥ng th·ª©c ƒë√≥ng nh∆∞ Linear Regression. Ta d√πng **gradient descent**:\n",
    "\n",
    "Gradient c·ªßa h√†m m·∫•t m√°t:\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{w} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i) \\cdot \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "\n",
    "## üí° V. T·ªïng k·∫øt pipeline m√¥ h√¨nh\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "A[ƒê·∫ßu v√†o: x] --> B[T√≠nh z = w^T x + b]\n",
    "B --> C[√Åp sigmoid: œÉ(z)]\n",
    "C --> D[So s√°nh v·ªõi y th·∫≠t]\n",
    "D --> E[T√≠nh Cross Entropy Loss]\n",
    "E --> F[C·∫≠p nh·∫≠t w, b b·∫±ng GD]\n",
    "```\n",
    "\n",
    "\n",
    "## üìä VI. ƒê√°nh gi√° m√¥ h√¨nh ph√¢n lo·∫°i\n",
    "\n",
    "| Metric        | √ù nghƒ©a                                     | D√πng khi                         |\n",
    "| ------------- | ------------------------------------------- | -------------------------------- |\n",
    "| **Accuracy**  | T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng                          | D·ªØ li·ªáu c√¢n b·∫±ng                 |\n",
    "| **Precision** | % ƒë√∫ng trong c√°c d·ª± ƒëo√°n d∆∞∆°ng              | Quan tr·ªçng khi mu·ªën √≠t d∆∞∆°ng gi·∫£ |\n",
    "| **Recall**    | % ƒë√∫ng trong t·∫•t c·∫£ m·∫´u d∆∞∆°ng               | Quan tr·ªçng khi mu·ªën √≠t √¢m gi·∫£    |\n",
    "| **F1-score**  | Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall | D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng           |\n",
    "| **ROC-AUC**   | ƒê√°nh gi√° kh·∫£ nƒÉng ph√¢n bi·ªát gi·ªØa hai l·ªõp    | So s√°nh m√¥ h√¨nh                  |\n",
    "\n",
    "\n",
    "## üõ°Ô∏è VII. Regularization\n",
    "\n",
    "Logistic Regression r·∫•t nh·∫°y v·ªõi **overfitting** khi c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng. Do ƒë√≥ th∆∞·ªùng d√πng th√™m:\n",
    "\n",
    "* **L1 regularization (Lasso)** ‚Üí lo·∫°i b·ªè ƒë·∫∑c tr∆∞ng kh√¥ng quan tr·ªçng\n",
    "* **L2 regularization (Ridge)** ‚Üí l√†m m∆∞·ª£t m√¥ h√¨nh\n",
    "\n",
    "T·ªïng loss s·∫Ω th√†nh:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reg}} = \\mathcal{L}_{\\text{BCE}} + \\lambda \\cdot \\text{Penalty}\n",
    "$$\n",
    "\n",
    "\n",
    "## üß™ VIII. Code th·ª±c h√†nh v·ªõi Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "**Note:** LogisticRegression trong sklearn m·∫∑c ƒë·ªãnh ƒë√£ c√≥ regularization (L2).\n",
    "\n",
    "\n",
    "## ‚úÖ ∆Øu - Nh∆∞·ª£c ƒëi·ªÉm\n",
    "\n",
    "| ∆Øu ƒëi·ªÉm                    | Nh∆∞·ª£c ƒëi·ªÉm                                    |\n",
    "| -------------------------- | --------------------------------------------- |\n",
    "| D·ªÖ hi·ªÉu, di·ªÖn gi·∫£i r√µ r√†ng | Kh√¥ng t·ªët v·ªõi quan h·ªá phi tuy·∫øn               |\n",
    "| Ch·∫°y nhanh, hi·ªáu qu·∫£ cao   | Nh·∫°y c·∫£m v·ªõi outlier                          |\n",
    "| C√≥ th·ªÉ th√™m regularization | D·ªÖ b·ªã ·∫£nh h∆∞·ªüng n·∫øu ƒë·∫∑c tr∆∞ng kh√¥ng chu·∫©n h√≥a |\n",
    "\n",
    "\n",
    "## üîç So s√°nh nhanh: Linear vs Logistic Regression\n",
    "\n",
    "| Ti√™u ch√≠      | Linear Regression        | Logistic Regression  |\n",
    "| ------------- | ------------------------ | -------------------- |\n",
    "| B√†i to√°n      | D·ª± ƒëo√°n s·ªë th·ª±c          | Ph√¢n lo·∫°i nh·ªã ph√¢n   |\n",
    "| Output        | $\\hat{y} \\in \\mathbb{R}$ | $\\hat{y} \\in (0,1)$  |\n",
    "| H√†m m·∫•t m√°t   | MSE                      | Binary Cross Entropy |\n",
    "| H√†m k√≠ch ho·∫°t | Kh√¥ng c√≥                 | Sigmoid              |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
