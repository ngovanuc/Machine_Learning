{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4510b869",
   "metadata": {},
   "source": [
    "**Softmax Regression**, hay c√≤n g·ªçi l√† **Multinomial Logistic Regression**, l√† **bi·∫øn th·ªÉ m·ªü r·ªông c·ªßa Logistic Regression** d√πng cho **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multiclass classification)**.\n",
    "\n",
    "\n",
    "## üß† I. T·ªïng quan\n",
    "\n",
    "Trong khi **Logistic Regression** x·ª≠ l√Ω **nh·ªã ph√¢n (2 l·ªõp)**, th√¨ **Softmax Regression** x·ª≠ l√Ω ƒë∆∞·ª£c **n l·ªõp (n > 2)**.\n",
    "\n",
    "\n",
    "## üìä II. √ù t∆∞·ªüng ch√≠nh\n",
    "\n",
    "V·ªõi Logistic Regression nh·ªã ph√¢n:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(\\mathbf{w}^\\top \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^\\top \\mathbf{x}}}\n",
    "$$\n",
    "\n",
    "‚Üí Output l√† 1 x√°c su·∫•t.\n",
    "\n",
    "V·ªõi Softmax Regression:\n",
    "\n",
    "* Ta t√≠nh **logit** cho m·ªói l·ªõp:\n",
    "\n",
    "$$\n",
    "z_k = \\mathbf{w}_k^\\top \\mathbf{x} + b_k \\quad \\text{v·ªõi } k = 1, 2, ..., K\n",
    "$$\n",
    "\n",
    "* Sau ƒë√≥ ƒë∆∞a qua **h√†m softmax**:\n",
    "\n",
    "$$\n",
    "P(y = k \\mid \\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "‚Üí K·∫øt qu·∫£ l√† **m·ªôt vector x√°c su·∫•t** v·ªõi t·ªïng = 1, ƒë·∫°i di·ªán cho x√°c su·∫•t thu·ªôc t·ª´ng l·ªõp.\n",
    "\n",
    "\n",
    "## üìâ III. H√†m m·∫•t m√°t: **Cross-Entropy Loss (multiclass)**\n",
    "\n",
    "Gi·∫£ s·ª≠ ta d√πng one-hot encoding cho nh√£n th·∫≠t $\\mathbf{y}$, th√¨:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{k=1}^{K} y_k \\log(P(y = k \\mid \\mathbf{x}))\n",
    "$$\n",
    "\n",
    "M√¥ h√¨nh s·∫Ω c·ªë t·ªëi ∆∞u ƒë·ªÉ x√°c su·∫•t ƒë√∫ng (c·ªßa nh√£n th·∫≠t) **g·∫ßn 1 nh·∫•t c√≥ th·ªÉ**.\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è IV. T·ªëi ∆∞u ho√°\n",
    "\n",
    "Gi·ªëng nh∆∞ logistic regression nh·ªã ph√¢n:\n",
    "\n",
    "* D√πng **Gradient Descent** ho·∫∑c **Stochastic Gradient Descent (SGD)**\n",
    "* C√≥ th·ªÉ th√™m **L2 regularization** ƒë·ªÉ tr√°nh overfitting\n",
    "\n",
    "\n",
    "## üõ†Ô∏è V. Code v√≠ d·ª• v·ªõi `scikit-learn`\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')  # d√πng softmax\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "## üß† VI. Softmax vs Logistic Regression\n",
    "\n",
    "| Ti√™u ch√≠      | Logistic Regression  | Softmax Regression       |\n",
    "| ------------- | -------------------- | ------------------------ |\n",
    "| Lo·∫°i b√†i to√°n | Nh·ªã ph√¢n             | Nhi·ªÅu l·ªõp                |\n",
    "| Output        | 1 x√°c su·∫•t           | Vector x√°c su·∫•t          |\n",
    "| K√≠ch ho·∫°t     | Sigmoid              | Softmax                  |\n",
    "| Loss          | Binary Cross-Entropy | Multiclass Cross-Entropy |\n",
    "\n",
    "\n",
    "## ‚úÖ ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm\n",
    "\n",
    "| ∆Øu ƒëi·ªÉm                                             | Nh∆∞·ª£c ƒëi·ªÉm                                |\n",
    "| --------------------------------------------------- | ----------------------------------------- |\n",
    "| D·ªÖ hi·ªÉu, di·ªÖn gi·∫£i x√°c su·∫•t                         | Kh√¥ng x·ª≠ l√Ω t·ªët d·ªØ li·ªáu phi tuy·∫øn         |\n",
    "| D·ªÖ hu·∫•n luy·ªán, ph·ªï bi·∫øn                             | C·∫ßn chu·∫©n h√≥a input ƒë·ªÉ t·ªëi ∆∞u t·ªët         |\n",
    "| L√† n·ªÅn t·∫£ng cho softmax layers trong neural network | Kh√¥ng h·ªçc t·ªët v·ªõi feature t∆∞∆°ng quan m·∫°nh |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
