{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b349bb",
   "metadata": {},
   "source": [
    "## üìò I. Linear Regression l√† g√¨?\n",
    "\n",
    "**Linear Regression** l√† m·ªôt thu·∫≠t to√°n h·ªçc c√≥ gi√°m s√°t (supervised learning) d√πng ƒë·ªÉ **d·ª± ƒëo√°n bi·∫øn li√™n t·ª•c** d·ª±a tr√™n m·ªôt ho·∫∑c nhi·ªÅu ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (feature).\n",
    "\n",
    "M√¥ h√¨nh gi·∫£ ƒë·ªãnh r·∫±ng **m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn l√† tuy·∫øn t√≠nh** ‚Äì t·ª©c ƒë·∫ßu ra l√† t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c ƒë·∫ßu v√†o.\n",
    "\n",
    "\n",
    "## üßÆ II. C√¥ng th·ª©c to√°n h·ªçc\n",
    "\n",
    "### 1. **Simple Linear Regression** (1 bi·∫øn ƒë·∫ßu v√†o):\n",
    "\n",
    "$$\n",
    "\\hat{y} = w x + b\n",
    "$$\n",
    "\n",
    "* $x$: ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o\n",
    "* $\\hat{y}$: gi√° tr·ªã d·ª± ƒëo√°n\n",
    "* $w$: h·ªá s·ªë g√≥c (tr·ªçng s·ªë)\n",
    "* $b$: intercept (bias)\n",
    "\n",
    "### 2. **Multiple Linear Regression** (n bi·∫øn):\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b = \\mathbf{Xw} + b\n",
    "$$\n",
    "\n",
    "* $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$: ma tr·∫≠n ƒë·∫∑c tr∆∞ng\n",
    "* $\\mathbf{w} \\in \\mathbb{R}^n$: vector tr·ªçng s·ªë\n",
    "\n",
    "\n",
    "## üéØ III. M·ª•c ti√™u h·ªçc\n",
    "\n",
    "T√¨m $\\mathbf{w}, b$ sao cho m√¥ h√¨nh d·ª± ƒëo√°n g·∫ßn ƒë√∫ng **gi√° tr·ªã th·∫≠t** nh·∫•t, t·ª©c l√† **gi·∫£m thi·ªÉu sai s·ªë**.\n",
    "\n",
    "\n",
    "## üìâ IV. H√†m m·∫•t m√°t (Loss Function)\n",
    "\n",
    "S·ª≠ d·ª•ng **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2 = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{w}^\\top \\mathbf{x}_i + b - y_i)^2\n",
    "$$\n",
    "\n",
    "* M·ª•c ti√™u l√† **t·ªëi ∆∞u (minimize)** h√†m m·∫•t m√°t n√†y\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è V. C√°ch t√¨m tham s·ªë\n",
    "\n",
    "C√≥ 2 c√°ch ph·ªï bi·∫øn:\n",
    "\n",
    "### 1. **Closed-form solution** (Normal Equation)\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "‚úÖ Ch√≠nh x√°c, nhanh v·ªõi t·∫≠p d·ªØ li·ªáu nh·ªè\n",
    "‚ùå T√≠nh ngh·ªãch ƒë·∫£o ma tr·∫≠n t·ªën th·ªùi gian v·ªõi t·∫≠p l·ªõn\n",
    "\n",
    "### 2. **Gradient Descent**\n",
    "\n",
    "C·∫≠p nh·∫≠t theo ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\cdot \\nabla_\\mathbf{w} \\mathcal{L}\n",
    "$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "\n",
    "* $\\alpha$: learning rate\n",
    "* Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{w} = \\frac{2}{m} \\mathbf{X}^\\top (\\mathbf{Xw} + b - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "‚úÖ D√πng ƒë∆∞·ª£c cho d·ªØ li·ªáu l·ªõn\n",
    "‚ùå C·∫ßn ch·ªçn learning rate h·ª£p l√Ω\n",
    "\n",
    "\n",
    "## üß† VI. Gi·∫£ ƒë·ªãnh c·ªßa Linear Regression\n",
    "\n",
    "Linear Regression c√≥ m·ªôt s·ªë **gi·∫£ ƒë·ªãnh quan tr·ªçng**:\n",
    "\n",
    "| Gi·∫£ ƒë·ªãnh                           | M√¥ t·∫£                                 |\n",
    "| ---------------------------------- | ------------------------------------- |\n",
    "| Tuy·∫øn t√≠nh                         | Quan h·ªá gi·ªØa X v√† y l√† tuy·∫øn t√≠nh     |\n",
    "| Kh√¥ng t·ª± t∆∞∆°ng quan                | C√°c l·ªói kh√¥ng ph·ª• thu·ªôc nhau          |\n",
    "| ƒê·ªìng ph∆∞∆°ng sai (Homoscedasticity) | ƒê·ªô l·ªách chu·∫©n c·ªßa l·ªói kh√¥ng thay ƒë·ªïi  |\n",
    "| Kh√¥ng ƒëa c·ªông tuy·∫øn                | C√°c bi·∫øn ƒë·∫ßu v√†o kh√¥ng qu√° t∆∞∆°ng quan |\n",
    "| Ph√¢n ph·ªëi chu·∫©n                    | L·ªói c√≥ ph√¢n ph·ªëi chu·∫©n (Gaussian)     |\n",
    "\n",
    "\n",
    "## üõ°Ô∏è VII. Regularization (Ridge/Lasso)\n",
    "\n",
    "ƒê·ªÉ tr√°nh **overfitting**, ta th√™m c√°c ƒëi·ªÅu ki·ªán ph·∫°t (penalty term):\n",
    "\n",
    "* **Ridge Regression** (L2):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ridge}} = \\mathcal{L} + \\lambda \\|\\mathbf{w}\\|_2^2\n",
    "$$\n",
    "\n",
    "* **Lasso Regression** (L1):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{lasso}} = \\mathcal{L} + \\lambda \\|\\mathbf{w}\\|_1\n",
    "$$\n",
    "\n",
    "| Regularization | ∆Øu ƒëi·ªÉm                                                    |\n",
    "| -------------- | ---------------------------------------------------------- |\n",
    "| L1 (Lasso)     | L√†m nh·ªè ho·∫∑c tri·ªát ti√™u b·ªõt tr·ªçng s·ªë ‚Äì gi√∫p ch·ªçn ƒë·∫∑c tr∆∞ng |\n",
    "| L2 (Ridge)     | L√†m nh·ªè c√°c tr·ªçng s·ªë ‚Äì gi·∫£m overfitting                    |\n",
    "\n",
    "\n",
    "## üìä VIII. ƒê√°nh gi√° m√¥ h√¨nh\n",
    "\n",
    "| Ch·ªâ s·ªë                    | √ù nghƒ©a                                         |\n",
    "| ------------------------- | ----------------------------------------------- |\n",
    "| MAE (Mean Absolute Error) | Sai s·ªë trung b√¨nh                               |\n",
    "| MSE / RMSE                | Ph·∫°t sai s·ªë l·ªõn n·∫∑ng h∆°n                        |\n",
    "| R¬≤ (R-squared)            | ƒêo m·ª©c ƒë·ªô gi·∫£i th√≠ch c·ªßa m√¥ h√¨nh (g·∫ßn 1 l√† t·ªët) |\n",
    "\n",
    "\n",
    "## üß™ IX. Th·ª±c h√†nh v·ªõi Scikit-learn (v√≠ d·ª• nhanh)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "## ‚úÖ T·ªïng k·∫øt\n",
    "\n",
    "| M·ª•c        | T√≥m t·∫Øt                               |\n",
    "| ---------- | ------------------------------------- |\n",
    "| C√¥ng th·ª©c  | $\\hat{y} = \\mathbf{Xw} + b$           |\n",
    "| M·ª•c ti√™u   | T·ªëi thi·ªÉu h√≥a sai s·ªë (MSE)            |\n",
    "| C√°ch gi·∫£i  | Normal Equation ho·∫∑c Gradient Descent |\n",
    "| V·∫•n ƒë·ªÅ     | Overfitting, multicollinearity        |\n",
    "| Gi·∫£i ph√°p  | Lasso, Ridge (Regularization)         |\n",
    "| ∆Øu ƒëi·ªÉm    | D·ªÖ hi·ªÉu, nhanh, di·ªÖn gi·∫£i t·ªët         |\n",
    "| Nh∆∞·ª£c ƒëi·ªÉm | Ch·ªâ h·ªçc ƒë∆∞·ª£c quan h·ªá tuy·∫øn t√≠nh       |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
