{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b2f992",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† danh s√°ch **to√†n di·ªán c√°c h√†m m·∫•t m√°t (loss functions)** ph·ªï bi·∫øn trong Machine Learning v√† Deep Learning, bao g·ªìm:\n",
    "\n",
    "* **Ph√¢n lo·∫°i (Classification)**\n",
    "* **H·ªìi quy (Regression)**\n",
    "* **T·ªëi ∆∞u h√≥a bi·ªÉu di·ªÖn (Representation Learning)**\n",
    "* **T·ª± m√£ h√≥a (Autoencoders)**\n",
    "* **Sinh m·∫´u (Generative Models)**\n",
    "* **H·ªçc tƒÉng c∆∞·ªùng (Reinforcement Learning)**\n",
    "* **Computer Vision**: segmentation, detection...\n",
    "* **Loss t√πy bi·∫øn kh√°c (custom/advanced)**\n",
    "\n",
    "\n",
    "## üìå I. H√ÄM M·∫§T M√ÅT CHO H·ªíI QUY (REGRESSION LOSSES)\n",
    "\n",
    "### 1. **Mean Squared Error (MSE)**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**:\n",
    "\n",
    "  * D·ªÖ t√≠nh to√°n, ph·ªï bi·∫øn.\n",
    "  * Khu·∫øch ƒë·∫°i l·ªói l·ªõn ‚Üí gi√∫p m√¥ h√¨nh h·ªçc t·ªët v·ªõi d·ªØ li·ªáu √≠t nhi·ªÖu.\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**:\n",
    "\n",
    "  * Nh·∫°y c·∫£m v·ªõi outliers.\n",
    "\n",
    "\n",
    "### 2. **Mean Absolute Error (MAE)**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{MAE}} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**:\n",
    "\n",
    "  * √çt nh·∫°y c·∫£m v·ªõi outliers h∆°n MSE.\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**:\n",
    "\n",
    "  * Kh√¥ng tr∆°n (non-differentiable) t·∫°i ƒëi·ªÉm 0 ‚Üí l√†m ch·∫≠m t·ªëi ∆∞u gradient.\n",
    "\n",
    "\n",
    "### 3. **Huber Loss**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_\\delta(y, \\hat{y}) = \n",
    "  \\begin{cases}\n",
    "    \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "    \\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**: K·∫øt h·ª£p ƒëi·ªÉm m·∫°nh c·ªßa MSE v√† MAE.\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**: C·∫ßn ch·ªçn hyperparameter $\\delta$.\n",
    "\n",
    "\n",
    "### 4. **Log-Cosh Loss**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{logcosh}} = \\sum \\log(\\cosh(\\hat{y} - y))\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**: M∆∞·ª£t m√† nh∆∞ MSE, b·ªÅn nh∆∞ MAE.\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**: T·ªën chi ph√≠ t√≠nh to√°n h∆°n MAE/MSE.\n",
    "\n",
    "\n",
    "## üìå II. PH√ÇN LO·∫†I (CLASSIFICATION)\n",
    "\n",
    "### 1. **Binary Cross-Entropy (Log Loss)**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)\\right]\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**: Ph√π h·ª£p cho b√†i to√°n nh·ªã ph√¢n, ƒë·∫ßu ra sigmoid.\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**: D·ªÖ b·ªã underflow n·∫øu kh√¥ng clip $\\hat{y}$.\n",
    "\n",
    "\n",
    "### 2. **Categorical Cross-Entropy**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{CCE}} = -\\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**: D√πng cho multi-class (softmax output).\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng d√πng cho nh√£n ch∆∞a one-hot.\n",
    "\n",
    "\n",
    "### 3. **Sparse Categorical Cross-Entropy**\n",
    "\n",
    "* Gi·ªëng CCE, nh∆∞ng nh√£n l√† index thay v√¨ one-hot ‚Üí nh·∫π h∆°n.\n",
    "\n",
    "\n",
    "### 4. **Kullback-Leibler Divergence (KL Divergence)**\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  D_{\\text{KL}}(P \\| Q) = \\sum P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "  $$\n",
    "* **∆Øu ƒëi·ªÉm**: D√πng so s√°nh ph√¢n ph·ªëi, √°p d·ª•ng trong distillation, VAE.\n",
    "* **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng ƒë·ªëi x·ª©ng, kh√¥ng ph·∫£i metric.\n",
    "\n",
    "\n",
    "## üìå III. H·ªåC BI·ªÇU DI·ªÑN (REPRESENTATION LEARNING)\n",
    "\n",
    "### 1. **Contrastive Loss**\n",
    "\n",
    "* D√†nh cho h·ªçc nh√∫ng (Siamese networks).\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L} = (1 - y) \\cdot D^2 + y \\cdot \\max(0, m - D)^2\n",
    "  $$\n",
    "\n",
    "  v·ªõi $D = \\|f(x_1) - f(x_2)\\|$\n",
    "\n",
    "\n",
    "### 2. **Triplet Loss**\n",
    "\n",
    "* So s√°nh anchor, positive, negative:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L} = \\max(0, \\|f(a) - f(p)\\|^2 - \\|f(a) - f(n)\\|^2 + \\alpha)\n",
    "  $$\n",
    "\n",
    "\n",
    "### 3. **NT-Xent Loss (SimCLR)**\n",
    "\n",
    "* Loss t∆∞∆°ng t·ª± nh∆∞ softmax, d√πng trong contrastive learning:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{i,j} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\ne i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "  $$\n",
    "\n",
    "\n",
    "## üìå IV. T·ª∞ M√É H√ìA (AUTOENCODER)\n",
    "\n",
    "### 1. **Reconstruction Loss**\n",
    "\n",
    "* D√πng MSE ho·∫∑c MAE gi·ªØa input v√† output:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{rec}} = \\|x - \\hat{x}\\|^2\n",
    "  $$\n",
    "\n",
    "### 2. **VAE Loss**\n",
    "\n",
    "* K·∫øt h·ª£p MSE v·ªõi KL Divergence:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{VAE}} = \\mathcal{L}_{\\text{rec}} + D_{\\text{KL}}(q(z|x) \\| p(z))\n",
    "  $$\n",
    "\n",
    "\n",
    "## üìå V. GENERATIVE MODELS\n",
    "\n",
    "### 1. **GAN Loss (Binary Cross-Entropy)**\n",
    "\n",
    "* Generator v√† discriminator ƒë·∫•u v·ªõi nhau:\n",
    "\n",
    "  * **Discriminator**:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{L}_D = -[\\log D(x) + \\log(1 - D(G(z)))]\n",
    "    $$\n",
    "  * **Generator**:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{L}_G = -\\log D(G(z))\n",
    "    $$\n",
    "\n",
    "\n",
    "## üìå VI. COMPUTER VISION CHUY√äN BI·ªÜT\n",
    "\n",
    "### 1. **IoU Loss / Dice Loss** (segmentation)\n",
    "\n",
    "* **Dice Loss**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{Dice}} = 1 - \\frac{2|X \\cap Y|}{|X| + |Y|}\n",
    "  $$\n",
    "* **IoU Loss**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{IoU}} = 1 - \\frac{|X \\cap Y|}{|X \\cup Y|}\n",
    "  $$\n",
    "\n",
    "\n",
    "### 2. **Focal Loss** (ƒë·ªëi ph√≥ m·∫•t c√¢n b·∫±ng l·ªõp)\n",
    "\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{focal}} = -\\alpha (1 - \\hat{y})^\\gamma \\log(\\hat{y})\n",
    "  $$\n",
    "\n",
    "\n",
    "## üìå VII. REINFORCEMENT LEARNING\n",
    "\n",
    "### 1. **Policy Gradient Loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\log \\pi_\\theta(a_t|s_t) \\cdot A_t\n",
    "$$\n",
    "\n",
    "\n",
    "### 2. **Value Function Loss** (critic):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\left(V(s) - R_t\\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "### 3. **PPO Loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t ) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "## üìå VIII. CUSTOM LOSS (ADVANCED)\n",
    "\n",
    "* **Perceptual Loss**: So s√°nh feature ·ªü layer trung gian c·ªßa CNN (VGG, ResNet).\n",
    "* **Earth Mover‚Äôs Distance (EMD)**: So s√°nh ph√¢n ph·ªëi.\n",
    "* **Cosine Embedding Loss**: D·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
