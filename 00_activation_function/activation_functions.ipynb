{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365df640",
   "metadata": {},
   "source": [
    "# Các hàm kích hoạt phổ biến"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867efbf",
   "metadata": {},
   "source": [
    "### 1. **Sigmoid**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "* **Giá trị đầu ra:** (0, 1)\n",
    "* **🎯 Dùng để đưa đầu ra về xác suất (binary classification).**\n",
    "* ✅ Ưu điểm:\n",
    "\n",
    "  * Ánh xạ đầu vào thành xác suất.\n",
    "* ❌ Nhược điểm:\n",
    "\n",
    "  * Gradient nhỏ khi $x$ lớn hoặc nhỏ → Vanishing gradient.\n",
    "  * Không zero-centered → khó tối ưu.\n",
    "* 📌 **Dùng ở output layer của mô hình binary classification.**\n",
    "\n",
    "\n",
    "### 2. **Tanh (Hyperbolic Tangent)**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\\sigma(2x) - 1\n",
    "  $$\n",
    "* **Giá trị đầu ra:** (−1, 1)\n",
    "* 🎯 **Giống Sigmoid nhưng zero-centered.**\n",
    "* ✅ Ưu điểm:\n",
    "\n",
    "  * Trung tâm ở 0 → tốt hơn sigmoid.\n",
    "* ❌ Nhược điểm:\n",
    "\n",
    "  * Vẫn bị vanishing gradient.\n",
    "* 📌 **Dùng ở hidden layers trong các mạng cổ điển.**\n",
    "\n",
    "\n",
    "### 3. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "* 🎯 **Đơn giản và hiệu quả, được dùng rộng rãi nhất.**\n",
    "* ✅ Ưu điểm:\n",
    "\n",
    "  * Tính toán nhanh, không saturate khi $x > 0$.\n",
    "  * Tránh vanishing gradient khi x > 0.\n",
    "* ❌ Nhược điểm:\n",
    "\n",
    "  * “Dying ReLU” (neuron chết khi $x < 0$).\n",
    "* 📌 **Sử dụng cho hidden layers trong CNN, DNN.**\n",
    "\n",
    "\n",
    "### 4. **Leaky ReLU**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x & \\text{if } x \\ge 0 \\\\ \\alpha x & \\text{if } x < 0 \\end{cases}\n",
    "  $$\n",
    "\n",
    "  với $\\alpha \\approx 0.01$\n",
    "* 🎯 **Giải quyết vấn đề Dying ReLU.**\n",
    "* ✅ Ưu điểm:\n",
    "\n",
    "  * Gradient nhỏ vẫn tồn tại khi $x < 0$.\n",
    "* ❌ Nhược điểm:\n",
    "\n",
    "  * $\\alpha$ cố định có thể không tối ưu.\n",
    "* 📌 **Thường dùng khi ReLU gây ra quá nhiều neurons chết.**\n",
    "\n",
    "\n",
    "### 5. **Parametric ReLU (PReLU)**\n",
    "\n",
    "* **Công thức:** giống Leaky ReLU, nhưng $\\alpha$ là học được.\n",
    "* 🎯 **Giúp mô hình học độ dốc phù hợp vùng âm.**\n",
    "* ✅ Tối ưu hơn Leaky ReLU.\n",
    "* ❌ Dễ overfit nếu dữ liệu nhỏ.\n",
    "* 📌 **Ứng dụng trong mô hình lớn: ResNet, VGG, GAN.**\n",
    "\n",
    "\n",
    "### 6. **ELU (Exponential Linear Unit)**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x & \\text{if } x \\ge 0 \\\\ \\alpha (e^x - 1) & \\text{if } x < 0 \\end{cases}\n",
    "  $$\n",
    "* 🎯 **Kết hợp lợi ích ReLU và smoothness.**\n",
    "* ✅ Tránh dying ReLU và gradient nhỏ hơn sigmoid.\n",
    "* ❌ Tính toán phức tạp hơn ReLU.\n",
    "* 📌 **Đôi khi dùng thay ReLU để có gradient ổn định hơn.**\n",
    "\n",
    "\n",
    "### 7. **Swish (Self-Gated Activation)**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = x \\cdot \\sigma(x)\n",
    "  $$\n",
    "* 🎯 **Tự điều chỉnh đầu ra, mượt mà hơn ReLU.**\n",
    "* ✅ Smooth, tránh dying neuron, tốt hơn ReLU trong nhiều bài toán.\n",
    "* ❌ Chậm hơn ReLU.\n",
    "* 📌 **Được dùng trong EfficientNet, Transformer.**\n",
    "\n",
    "\n",
    "### 8. **GELU (Gaussian Error Linear Unit)**\n",
    "\n",
    "* **Công thức (xấp xỉ):**\n",
    "\n",
    "  $$\n",
    "  f(x) = 0.5x(1 + \\tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))\n",
    "  $$\n",
    "* 🎯 **Mượt và tự nhiên hơn ReLU.**\n",
    "* ✅ Tốt cho transformer (GPT, BERT).\n",
    "* ❌ Tính toán nặng.\n",
    "* 📌 **Dùng trong GPT-2, BERT.**\n",
    "\n",
    "\n",
    "### 9. **Softmax**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "  $$\n",
    "* 🎯 **Biến vector thành xác suất phân phối (tổng = 1).**\n",
    "* ✅ Phù hợp với classification multi-class.\n",
    "* ❌ Không dùng ở hidden layers.\n",
    "* 📌 **Dùng ở output layer của multi-class classifier.**\n",
    "\n",
    "\n",
    "### 10. **Hard Sigmoid / Hard Tanh**\n",
    "\n",
    "* **Công thức:** Xấp xỉ sigmoid và tanh bằng đoạn thẳng để giảm chi phí tính toán.\n",
    "* 🎯 **Dùng trong thiết bị di động hoặc khi cần nhanh.**\n",
    "* ✅ Tốc độ nhanh.\n",
    "* ❌ Không smooth → khó học tốt trong vài bài toán.\n",
    "* 📌 **Thường dùng trong embedded hoặc mobile networks.**\n",
    "\n",
    "\n",
    "### 11. **Maxout**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\max(w_1^Tx + b_1, w_2^Tx + b_2)\n",
    "  $$\n",
    "* 🎯 **Không cố định dạng hàm phi tuyến.**\n",
    "* ✅ Linh hoạt, tránh dying ReLU.\n",
    "* ❌ Tốn nhiều tham số.\n",
    "* 📌 **Dùng trong mạng deep khi cần tính linh hoạt cao.**\n",
    "\n",
    "\n",
    "### 12. **Mish**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = x \\cdot \\tanh(\\ln(1 + e^x))\n",
    "  $$\n",
    "* 🎯 **Kết hợp ưu điểm ReLU, Swish, Tanh.**\n",
    "* ✅ Smooth, zero-centered, không bị dying neuron.\n",
    "* ❌ Tính toán nặng.\n",
    "* 📌 **Dùng trong các bài toán thị giác máy tính hiện đại.**\n",
    "\n",
    "\n",
    "### 13. **Thresholded ReLU**\n",
    "\n",
    "* **Công thức:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x & \\text{if } x > \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "  $$\n",
    "* 🎯 Chỉ cho phép truyền gradient khi vượt ngưỡng.\n",
    "* 📌 **Thường dùng trong sparse coding.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
