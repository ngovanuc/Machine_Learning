{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365df640",
   "metadata": {},
   "source": [
    "# CÃ¡c hÃ m kÃ­ch hoáº¡t phá»• biáº¿n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867efbf",
   "metadata": {},
   "source": [
    "### 1. **Sigmoid**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "* **GiÃ¡ trá»‹ Ä‘áº§u ra:** (0, 1)\n",
    "* **ğŸ¯ DÃ¹ng Ä‘á»ƒ Ä‘Æ°a Ä‘áº§u ra vá» xÃ¡c suáº¥t (binary classification).**\n",
    "* âœ… Æ¯u Ä‘iá»ƒm:\n",
    "\n",
    "  * Ãnh xáº¡ Ä‘áº§u vÃ o thÃ nh xÃ¡c suáº¥t.\n",
    "* âŒ NhÆ°á»£c Ä‘iá»ƒm:\n",
    "\n",
    "  * Gradient nhá» khi $x$ lá»›n hoáº·c nhá» â†’ Vanishing gradient.\n",
    "  * KhÃ´ng zero-centered â†’ khÃ³ tá»‘i Æ°u.\n",
    "* ğŸ“Œ **DÃ¹ng á»Ÿ output layer cá»§a mÃ´ hÃ¬nh binary classification.**\n",
    "\n",
    "\n",
    "### 2. **Tanh (Hyperbolic Tangent)**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\\sigma(2x) - 1\n",
    "  $$\n",
    "* **GiÃ¡ trá»‹ Ä‘áº§u ra:** (âˆ’1, 1)\n",
    "* ğŸ¯ **Giá»‘ng Sigmoid nhÆ°ng zero-centered.**\n",
    "* âœ… Æ¯u Ä‘iá»ƒm:\n",
    "\n",
    "  * Trung tÃ¢m á»Ÿ 0 â†’ tá»‘t hÆ¡n sigmoid.\n",
    "* âŒ NhÆ°á»£c Ä‘iá»ƒm:\n",
    "\n",
    "  * Váº«n bá»‹ vanishing gradient.\n",
    "* ğŸ“Œ **DÃ¹ng á»Ÿ hidden layers trong cÃ¡c máº¡ng cá»• Ä‘iá»ƒn.**\n",
    "\n",
    "\n",
    "### 3. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "* ğŸ¯ **ÄÆ¡n giáº£n vÃ  hiá»‡u quáº£, Ä‘Æ°á»£c dÃ¹ng rá»™ng rÃ£i nháº¥t.**\n",
    "* âœ… Æ¯u Ä‘iá»ƒm:\n",
    "\n",
    "  * TÃ­nh toÃ¡n nhanh, khÃ´ng saturate khi $x > 0$.\n",
    "  * TrÃ¡nh vanishing gradient khi x > 0.\n",
    "* âŒ NhÆ°á»£c Ä‘iá»ƒm:\n",
    "\n",
    "  * â€œDying ReLUâ€ (neuron cháº¿t khi $x < 0$).\n",
    "* ğŸ“Œ **Sá»­ dá»¥ng cho hidden layers trong CNN, DNN.**\n",
    "\n",
    "\n",
    "### 4. **Leaky ReLU**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x & \\text{if } x \\ge 0 \\\\ \\alpha x & \\text{if } x < 0 \\end{cases}\n",
    "  $$\n",
    "\n",
    "  vá»›i $\\alpha \\approx 0.01$\n",
    "* ğŸ¯ **Giáº£i quyáº¿t váº¥n Ä‘á» Dying ReLU.**\n",
    "* âœ… Æ¯u Ä‘iá»ƒm:\n",
    "\n",
    "  * Gradient nhá» váº«n tá»“n táº¡i khi $x < 0$.\n",
    "* âŒ NhÆ°á»£c Ä‘iá»ƒm:\n",
    "\n",
    "  * $\\alpha$ cá»‘ Ä‘á»‹nh cÃ³ thá»ƒ khÃ´ng tá»‘i Æ°u.\n",
    "* ğŸ“Œ **ThÆ°á»ng dÃ¹ng khi ReLU gÃ¢y ra quÃ¡ nhiá»u neurons cháº¿t.**\n",
    "\n",
    "\n",
    "### 5. **Parametric ReLU (PReLU)**\n",
    "\n",
    "* **CÃ´ng thá»©c:** giá»‘ng Leaky ReLU, nhÆ°ng $\\alpha$ lÃ  há»c Ä‘Æ°á»£c.\n",
    "* ğŸ¯ **GiÃºp mÃ´ hÃ¬nh há»c Ä‘á»™ dá»‘c phÃ¹ há»£p vÃ¹ng Ã¢m.**\n",
    "* âœ… Tá»‘i Æ°u hÆ¡n Leaky ReLU.\n",
    "* âŒ Dá»… overfit náº¿u dá»¯ liá»‡u nhá».\n",
    "* ğŸ“Œ **á»¨ng dá»¥ng trong mÃ´ hÃ¬nh lá»›n: ResNet, VGG, GAN.**\n",
    "\n",
    "\n",
    "### 6. **ELU (Exponential Linear Unit)**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x & \\text{if } x \\ge 0 \\\\ \\alpha (e^x - 1) & \\text{if } x < 0 \\end{cases}\n",
    "  $$\n",
    "* ğŸ¯ **Káº¿t há»£p lá»£i Ã­ch ReLU vÃ  smoothness.**\n",
    "* âœ… TrÃ¡nh dying ReLU vÃ  gradient nhá» hÆ¡n sigmoid.\n",
    "* âŒ TÃ­nh toÃ¡n phá»©c táº¡p hÆ¡n ReLU.\n",
    "* ğŸ“Œ **ÄÃ´i khi dÃ¹ng thay ReLU Ä‘á»ƒ cÃ³ gradient á»•n Ä‘á»‹nh hÆ¡n.**\n",
    "\n",
    "\n",
    "### 7. **Swish (Self-Gated Activation)**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = x \\cdot \\sigma(x)\n",
    "  $$\n",
    "* ğŸ¯ **Tá»± Ä‘iá»u chá»‰nh Ä‘áº§u ra, mÆ°á»£t mÃ  hÆ¡n ReLU.**\n",
    "* âœ… Smooth, trÃ¡nh dying neuron, tá»‘t hÆ¡n ReLU trong nhiá»u bÃ i toÃ¡n.\n",
    "* âŒ Cháº­m hÆ¡n ReLU.\n",
    "* ğŸ“Œ **ÄÆ°á»£c dÃ¹ng trong EfficientNet, Transformer.**\n",
    "\n",
    "\n",
    "### 8. **GELU (Gaussian Error Linear Unit)**\n",
    "\n",
    "* **CÃ´ng thá»©c (xáº¥p xá»‰):**\n",
    "\n",
    "  $$\n",
    "  f(x) = 0.5x(1 + \\tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))\n",
    "  $$\n",
    "* ğŸ¯ **MÆ°á»£t vÃ  tá»± nhiÃªn hÆ¡n ReLU.**\n",
    "* âœ… Tá»‘t cho transformer (GPT, BERT).\n",
    "* âŒ TÃ­nh toÃ¡n náº·ng.\n",
    "* ğŸ“Œ **DÃ¹ng trong GPT-2, BERT.**\n",
    "\n",
    "\n",
    "### 9. **Softmax**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "  $$\n",
    "* ğŸ¯ **Biáº¿n vector thÃ nh xÃ¡c suáº¥t phÃ¢n phá»‘i (tá»•ng = 1).**\n",
    "* âœ… PhÃ¹ há»£p vá»›i classification multi-class.\n",
    "* âŒ KhÃ´ng dÃ¹ng á»Ÿ hidden layers.\n",
    "* ğŸ“Œ **DÃ¹ng á»Ÿ output layer cá»§a multi-class classifier.**\n",
    "\n",
    "\n",
    "### 10. **Hard Sigmoid / Hard Tanh**\n",
    "\n",
    "* **CÃ´ng thá»©c:** Xáº¥p xá»‰ sigmoid vÃ  tanh báº±ng Ä‘oáº¡n tháº³ng Ä‘á»ƒ giáº£m chi phÃ­ tÃ­nh toÃ¡n.\n",
    "* ğŸ¯ **DÃ¹ng trong thiáº¿t bá»‹ di Ä‘á»™ng hoáº·c khi cáº§n nhanh.**\n",
    "* âœ… Tá»‘c Ä‘á»™ nhanh.\n",
    "* âŒ KhÃ´ng smooth â†’ khÃ³ há»c tá»‘t trong vÃ i bÃ i toÃ¡n.\n",
    "* ğŸ“Œ **ThÆ°á»ng dÃ¹ng trong embedded hoáº·c mobile networks.**\n",
    "\n",
    "\n",
    "### 11. **Maxout**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\max(w_1^Tx + b_1, w_2^Tx + b_2)\n",
    "  $$\n",
    "* ğŸ¯ **KhÃ´ng cá»‘ Ä‘á»‹nh dáº¡ng hÃ m phi tuyáº¿n.**\n",
    "* âœ… Linh hoáº¡t, trÃ¡nh dying ReLU.\n",
    "* âŒ Tá»‘n nhiá»u tham sá»‘.\n",
    "* ğŸ“Œ **DÃ¹ng trong máº¡ng deep khi cáº§n tÃ­nh linh hoáº¡t cao.**\n",
    "\n",
    "\n",
    "### 12. **Mish**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = x \\cdot \\tanh(\\ln(1 + e^x))\n",
    "  $$\n",
    "* ğŸ¯ **Káº¿t há»£p Æ°u Ä‘iá»ƒm ReLU, Swish, Tanh.**\n",
    "* âœ… Smooth, zero-centered, khÃ´ng bá»‹ dying neuron.\n",
    "* âŒ TÃ­nh toÃ¡n náº·ng.\n",
    "* ğŸ“Œ **DÃ¹ng trong cÃ¡c bÃ i toÃ¡n thá»‹ giÃ¡c mÃ¡y tÃ­nh hiá»‡n Ä‘áº¡i.**\n",
    "\n",
    "\n",
    "### 13. **Thresholded ReLU**\n",
    "\n",
    "* **CÃ´ng thá»©c:**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x & \\text{if } x > \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "  $$\n",
    "* ğŸ¯ Chá»‰ cho phÃ©p truyá»n gradient khi vÆ°á»£t ngÆ°á»¡ng.\n",
    "* ğŸ“Œ **ThÆ°á»ng dÃ¹ng trong sparse coding.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
